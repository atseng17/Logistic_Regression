{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Logistic Regression?\n",
    "Logistic regression is the appropriate regression analysis to conduct when the dependent variable is binary. Logistic regression generates a probabilityâ€”a value between 0 and 1. When we are dealing with a problem such as spam email detection with linear regression, we have to put a threshold. So that whenever the predicted value is larger than the threshold, we say that email is a spam email. Problem appears when we have significant differences between the predicted values, the threshold would be sensitive to the values. Thus we say that linear regression is unbounded. Logistic regression strictly constraints the predicted values between 0 and 1.\n",
    "\n",
    "In this tutorial, we are going to see how to do logictic regression using scikit learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load our data. The first two rows are the parameters and the third column is the dependent value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/enespolat/grid-search-with-logistic-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.datasets import load_iris\n",
    "# print(os.listdir(\"./input\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load out dataset.\n",
    "Note that iris dataset is s dictionary, we set x and y as the data and the target.\n",
    "And then we split the data in to test and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris=load_iris()#print(iris)\n",
    "x=iris.data # print(x.shape)\n",
    "y=iris.target # print(y.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clealy seen that there are five features for eath Iris flower data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.6 3.2 1.4 0.2]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.6 2.5 3.9 1.1]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (Scaling)\n",
    "Next, we apply scaling on the test dataset, and apply the same transform to the test dataset.\n",
    "After fitting, the scale and offset used with your training data is stored. We use that on the test dataset with scaler.transform.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform( x_train )\n",
    "x_test = scaler.transform( x_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation with Regularization\n",
    "Any model has its own hyperparameter. These paramters could change the training results significantly. Thus we have to do hyper paramter tuning via cross validation.\n",
    "\n",
    "One method is called grid search cross validation. One exhaustively navigates through the n parameters values (the grids). So we have a n squared parameter combination.\n",
    "\n",
    "The grid term is the parameter set where we include all the hyper parameters that the model uses and the range of values that we would like to test. In logistic regression, there are two hyperparameters we could tune, one is the regularization coefficient C and the second is the regularization method L.\n",
    "\n",
    "Finally, one uses K-folds cross validation to test the accuracy of the model. the cv argument in GridSearchCV allows one to decide how many folds one wants to use. an average score for each model is then generated. Only the best model and its hyperparameters are shown in the end.\n",
    "\n",
    "source:\n",
    "1. https://www.youtube.com/watch?v=IXPgm1e0IOo\n",
    "2. https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned hpyerparameters :(best parameters)  {'C': 10.0, 'penalty': 'l1'}\n",
      "accuracy : 0.9619047619047619\n"
     ]
    }
   ],
   "source": [
    "# Simply remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Grid search cross validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "logreg=LogisticRegression()\n",
    "logreg_cv=GridSearchCV(logreg,grid,cv=10)\n",
    "logreg_cv.fit(x_train,y_train)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "print(\"accuracy :\",logreg_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "Finally we do predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "logreg2=LogisticRegression(C=1,penalty=\"l2\")\n",
    "logreg2.fit(x_train,y_train)\n",
    "print(\"score\",logreg2.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P.S.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One could manually find the cv scores by the following lines. This example shows when we have selected C=10 and L1 morn as regularization, the average score of the 10 folds cross validation is 96.16%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9616666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "print(cross_val_score(LogisticRegression(C=10,penalty=\"l1\"), x_train, y_train, scoring='accuracy', cv = 10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
